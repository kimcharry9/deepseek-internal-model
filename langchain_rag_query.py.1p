import json
import pandas as pd
import glob
import threading
import time
import torch
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains.llm import LLMChain
from langchain.chains import RetrievalQA
from langchain.schema import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain import hub
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.docstore.document import Document
from langchain_community.vectorstores import Chroma

model_dir = "/data/deepseek-model/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"
data_dir = "/data/deepseek-model/cleansed_dataset"
db_dir = "/data/deepseek-model/faiss_db"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

######################################################### 
#   RAG Processing                                      #
#########################################################

#-- Load Datasets to Document Format --#
date_range = [str(i) for i in range(20250218, 20250219)]

file_paths = []
for date in date_range:
    file_paths.extend(glob.glob(f"/data/deepseek-model/mockdata/{date}/*gsb-lake-prd-monitor02_8086_telegraf_disk.csv.gz"))

df_list = [pd.read_csv(file, compression='gzip') for file in tqdm(file_paths, desc="Reading CSV files to pandas", unit="file")]
df_list = pd.concat(df_list, ignore_index=True)
df_list = df_list.dropna()

# แปลง timestamp เป็น datetime format
df_list["time"] = df_list["time"].astype(str)

datasets = []

for idx, (_, row) in enumerate(tqdm(df_list.iterrows(), total=len(df_list), desc="Generating Datasets", unit="rows")):
    timestamp = row["time"]
    disk_path = row["path"]
    hostname = row["hostname"]
    disk_used_percent = float(row["disk_used.percent"])
    disk_total = int(row["disk_total.bytes"])
    disk_used_bytes = int(row["disk_used.bytes"])

    text_to_embed = f"At {timestamp}, disk path '{disk_path}' from host '{hostname}' has used {disk_used_percent:.2f}% of total disk allocation: {disk_used_bytes} of {disk_total} bytes."

    datasets.append(text_to_embed)

    #progress["current"] = idx + 1

#-- [Not working] Load Datasets from FAISS DB --#
#embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#vector_db = FAISS.load_local(db_dir, embeddings, allow_dangerous_deserialization=True)

#-- Generate Datasets via Chroma into vectorstore --#
documents = [Document(page_content=text) for text in datasets]
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=200
)
splits = text_splitter.split_documents(documents)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Documents Importing Progress Function
progress = {
    "current": 0,
    "total": len(splits)
}
def progress_checker():
    while progress["current"] < progress["total"]:
        print(f"[Progress] Processed {progress['current']} / {progress['total']} rows")
        time.sleep(5)
    print("[Progress] Done processing all rows!")
progress_thread = threading.Thread(target=progress_checker)
progress_thread.start()

def embed_document(doc):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embeddings.embed(doc)

with ThreadPoolExecutor(max_workers=8) as executor:
    embeddings_list = list(executor.map(lambda doc: embed_document(doc.page_content), documents))

def embed_docs_with_progress(splits):
    embedded_docs = []
    for doc in tqdm(splits, desc="Embedding documents", unit="document"):  # ใช้ tqdm ติดตามขั้นตอนนี้ด้วย
        _ = embeddings.embed_query(doc.page_content)  # ฝัง embeddings สำหรับแต่ละ document
        progress["current"] += 1  # อัพเดต progress
        embedded_docs.append(doc)  # เก็บเอกสารที่ฝังแล้ว
    return embedded_docs

splits = embed_docs_with_progress(splits)

# หยุด progress thread หลังจากเสร็จสิ้น
progress_thread.join()

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)
print("Vectorstore created successfully!")

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Import Private Model via Ollama
model = Ollama(model="deepseek-r1-8b-param-ver20250321_145325")

# Craft the prompt template
prompt = hub.pull("rlm/rag-prompt")

# Chain 1: Generate answers
llm_chain = LLMChain(llm=model, prompt=prompt)

# Chain 2: Combine document chunks
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ใช้ RunnableLambda เพื่อเปลี่ยนเอกสารเป็น string
formatted_retriever = retriever | format_docs

rag_chain = (
    {"context": formatted_retriever, "question": RunnablePassthrough()}
    | prompt
    | llm_chain
    | StrOutputParser()
)

result = rag_chain.invoke({"question": "What is the percent disk_usage of disk path '/var' at 2025-02-12T03:00:00+07:00 from host 'gsb-lake-prd-app12'"})
print(result)
