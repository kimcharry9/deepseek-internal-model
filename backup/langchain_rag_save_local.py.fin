import json
import pandas as pd
import glob
import time
import torch
import faiss
from uuid import uuid4
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

model_dir = "/data/deepseek-model/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"
data_dir = "/data/deepseek-model/cleansed_dataset"
db_dir = "/data/deepseek-model/faiss_db"
embedding_model = HuggingFaceEmbeddings()

######################################################### 
#   RAG Processing                                      #
#########################################################

index = faiss.IndexFlatL2(len(embedding_model.embed_query("hello world")))

current_db = FAISS(
    embedding_function=embedding_model,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

#-- Load Datasets to Document Format --#
date_range = [str(i) for i in range(20250218, 20250219)]

file_paths = []
for date in date_range:
    file_paths.extend(glob.glob(f"/data/deepseek-model/mockdata/{date}/*gsb-lake-prd-monitor02_8086_telegraf_disk.csv.gz"))

with tqdm(file_paths, desc=f"Processing Files", unit="files") as raw_data:
    for file in raw_data:
        df = pd.read_csv(file, compression='gzip').dropna()
        df["time"] = pd.to_datetime(df['time'])  # แปลง timestamp

        with tqdm(df.iterrows(), total=len(df), desc="- Ingesting documents", unit="rows") as datasets:
            for _, row in datasets:
                doc = Document(
                    page_content=f"Percent of disk_usage for '{row['hostname']}' on '{row['time'].date().isoformat()}' at '{row['time'].time().isoformat()}' for path '{row['path']}' has used {row['disk_used.percent']:.2f}% of total disk allocation: {int(row['disk_used.bytes'])} of {int(row['disk_total.bytes'])} bytes.",
                    metadata={
                        "date": row["time"].date().isoformat(),
                        "time": row["time"].time().isoformat(),
                        "hostname": row["hostname"],
                        "disk_path": row["path"],
                        "disk_used_percent": float(row["disk_used.percent"]),
                        "disk_total": int(row["disk_total.bytes"]),
                        "disk_used_bytes": int(row["disk_used.bytes"])
                    },
                )
                #uuids = str(uuid4()) for _ in range(len(datasets))
                current_db.add_documents(documents=[doc])
                #if current_db:
                #    current_db.add_documents(documents=[doc])
                #else:
                #    current_db = FAISS.from_documents([doc], embedding_model)
                datasets.update(1)

#-- Generate Datasets via Chroma into vectorstore --#
#text_splitter = RecursiveCharacterTextSplitter(
#    chunk_size=1000,
#    chunk_overlap=200
#)
#splits = text_splitter.split_documents(documents)

current_db.save_local(db_dir)
print("Vectorstore created successfully!")
